{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Setup ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import pandas\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "import sklearn.model_selection\n",
    "import transformers\n",
    "\n",
    "sys.path.append(\"../source\")\n",
    "import document\n",
    "import data_preprocessing\n",
    "import transformer_model\n",
    "\n",
    "pandas.set_option(\"display.max_rows\", None)\n",
    "pandas.set_option(\"display.max_columns\", None)\n",
    "pandas.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gc = pandas.read_csv(\"../datasets/green_claims.csv\")\n",
    "print(\"Dataset Size:\", df_gc.shape)\n",
    "df_gc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_so = pandas.read_csv(\"../datasets/sustainability_goals.csv\")\n",
    "df_so = df_so.dropna(subset=[\"Text Blocks\", \"Goal\"])\n",
    "df_so = df_so.drop_duplicates(subset=[\"Text Blocks\"])\n",
    "\n",
    "print(\"Dataset Size:\", df_so.shape)\n",
    "print(\"The Number of Goals:\", df_so[\"Goal\"].sum())\n",
    "df_so.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sustainability_keywords = [\n",
    "    \"green\", \"environment\", \"carbon\", \"footprint\", \"co2\",  \"emission\", \"pollution\", \"recycle\", \"waste\", \"plant\", \"energy\", \"renewable\", \"water\", \"electricity\",\n",
    "    \"diversity\", \"employee\", \"women\", \"female\", \"human\", \"inclusion\", \"health\", \"safety\", \"security\",\n",
    "    # \"goal\", \"sustainable\", \"zero\", \"right\"\n",
    "    ]\n",
    "data_preprocessor = data_preprocessing.DataPreprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gc[\"text\"] = df_gc[\"tweet\"].copy()\n",
    "# df_gc = data_preprocessor.clean_text_blocks(df_gc, \"text\", level=\"heavy\")\n",
    "df_gc[\"labels\"] = df_gc[\"label_binary\"].replace({\"not_green\": 0, \"green_claim\": 1})\n",
    "df_gc = df_gc[[\"text\", \"labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_so[\"text\"] = df_so[\"Text Blocks\"].copy()\n",
    "# df_so = data_preprocessor.clean_text_blocks(df_so, \"text\", level=\"heavy\")\n",
    "df_so = data_preprocessor.filter_text_blocks(df_so, \"text\", keep_only_size=(0, 300), keep_only_keywords=sustainability_keywords)\n",
    "df_so[\"labels\"] = df_so[\"Goal\"].copy()\n",
    "df_so = df_so[[\"text\", \"labels\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gc_train, df_gc_test = sklearn.model_selection.train_test_split(\n",
    "    df_gc,\n",
    "    test_size=0.2,\n",
    "    stratify=df_gc[\"labels\"],\n",
    "    random_state=7\n",
    ")\n",
    "print(\"Train Set Size:\", df_gc_train.shape)\n",
    "print(df_gc_train[\"labels\"].value_counts())\n",
    "print(\"Test Set Size:\", df_gc_test.shape)\n",
    "print(df_gc_test[\"labels\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_so_train, df_so_test = sklearn.model_selection.train_test_split(\n",
    "    df_so,\n",
    "    test_size=0.2,\n",
    "    stratify=df_so[\"labels\"],\n",
    "    random_state=7\n",
    ")\n",
    "\n",
    "print(\"Train Set Size:\", df_so_train.shape)\n",
    "print(df_so_train[\"labels\"].value_counts())\n",
    "print(\"Test Set Size:\", df_so_test.shape)\n",
    "print(df_so_test[\"labels\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Baseline 1: BERTClaimBuster ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"Nithiwat/bert-base_claimbuster\")\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"Nithiwat/bert-base_claimbuster\", num_labels=3)#.to(device) \n",
    "pipe = transformers.TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "print(time.time() - t)\n",
    "predictions = pipe(df_gc_test[\"text\"].tolist())\n",
    "y_predicted = [(p[2][\"score\"] >= p[0][\"score\"]) and (p[2][\"score\"] >= p[1][\"score\"]) for p in predictions]\n",
    "evaluation_metrics = sklearn.metrics.classification_report(df_gc_test[\"labels\"], y_predicted)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"Nithiwat/bert-base_claimbuster\")\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"Nithiwat/bert-base_claimbuster\", num_labels=3)#.to(device) \n",
    "pipe = transformers.TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "print(time.time() - t)\n",
    "predictions = pipe(df_so_test[\"text\"].tolist())\n",
    "y_predicted = [(p[2][\"score\"] >= p[0][\"score\"]) and (p[2][\"score\"] >= p[1][\"score\"]) for p in predictions]\n",
    "evaluation_metrics = sklearn.metrics.classification_report(df_so_test[\"labels\"], y_predicted)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Baseline 2: TFIDF + Random Forest ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(min_df=3, max_df=0.5, analyzer=\"word\")\n",
    "x_train_vectorized = vectorizer.fit_transform(df_gc_train[\"text\"])\n",
    "x_test_vectorized = vectorizer.transform(df_gc_test[\"text\"])\n",
    "parameters_grid = {\"n_estimators\": range(50, 550, 50)}\n",
    "model = sklearn.model_selection.GridSearchCV(sklearn.ensemble.RandomForestClassifier(), \n",
    "                                             parameters_grid, scoring=\"f1_macro\", cv=4, n_jobs=-1)\n",
    "model.fit(x_train_vectorized, df_gc_train[\"labels\"])\n",
    "print(time.time() - t)\n",
    "print(\"Best found hyperparameters of Random Forest classfier = {}\".format(model.best_params_))\n",
    "y_predicted = model.predict(x_test_vectorized)\n",
    "evaluation_metrics = sklearn.metrics.classification_report(df_gc_test[\"labels\"], y_predicted)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(min_df=3, max_df=0.5, analyzer=\"word\")\n",
    "x_train_vectorized = vectorizer.fit_transform(df_so_train[\"text\"])\n",
    "x_test_vectorized = vectorizer.transform(df_so_test[\"text\"])\n",
    "parameters_grid = {\"n_estimators\": range(50, 550, 50)}\n",
    "model = sklearn.model_selection.GridSearchCV(sklearn.ensemble.RandomForestClassifier(), \n",
    "                                             parameters_grid, scoring=\"f1_macro\", cv=4, n_jobs=-1)\n",
    "model.fit(x_train_vectorized, df_so_train[\"labels\"])\n",
    "print(\"Best found hyperparameters of Random Forest classfier = {}\".format(model.best_params_))\n",
    "print(time.time() - t)\n",
    "y_predicted = model.predict(x_test_vectorized)\n",
    "evaluation_metrics = sklearn.metrics.classification_report(df_so_test[\"labels\"], y_predicted)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Baseline 3: Bin_RoBERTa ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "target_values = [\"Not Green\", \"Green\"]\n",
    "model = transformer_model.TextClassification(target_values, name=\"roberta-base\", epochs=10, learning_rate=2e-5, batch_size=32, \n",
    "                                             weight_decay=0.0, save_to=\"../models/temp\")\n",
    "model.fit(df_gc_train, df_gc_test)\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "target_values = [\"Not Green\", \"Green\"]\n",
    "model = transformer_model.TextClassification(target_values, name=\"roberta-base\", epochs=10, learning_rate=2e-5, batch_size=32, \n",
    "                                             weight_decay=0.0, save_to=\"../models/temp\")\n",
    "model.fit(df_so_train, df_so_test)\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Training and Testing Our Model on the Dataset ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "target_values = [\"Not Green\", \"Green\"]\n",
    "model = transformer_model.TextClassification(target_values, name=\"distilroberta-base\", epochs=10, learning_rate=5e-5, batch_size=16, \n",
    "                                             weight_decay=0.01, save_to=\"../models/temp\")\n",
    "model.fit(df_gc_train, df_gc_test)\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "target_values = [\"Not Green\", \"Green\"]\n",
    "model = transformer_model.TextClassification(target_values, name=\"distilroberta-base\", epochs=10, learning_rate=5e-5, batch_size=16, \n",
    "                                             weight_decay=0.01, save_to=\"../models/temp\")\n",
    "model.fit(df_so_train, df_so_test)\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
