{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Setup ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "import sklearn.model_selection\n",
    "\n",
    "sys.path.append(\"../source\")\n",
    "import document\n",
    "import data_preprocessing\n",
    "import transformer_model\n",
    "\n",
    "pandas.set_option(\"display.max_rows\", None)\n",
    "pandas.set_option(\"display.max_columns\", None)\n",
    "pandas.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gc = pandas.read_csv(\"../datasets/green_claims.csv\")\n",
    "print(\"Dataset Size:\", df_gc.shape)\n",
    "df_gc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_so = pandas.read_csv(\"../datasets/sustainability_goals.csv\")\n",
    "df_so = df_so.dropna(subset=[\"Text Blocks\"])\n",
    "\n",
    "print(\"Dataset Size:\", df_so.shape)\n",
    "print(\"The Number of Goals:\", df_so[\"Goal\"].sum())\n",
    "df_so.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sustainability_keywords = [\n",
    "    \"green\", \"environment\", \"carbon\", \"footprint\", \"co2\",  \"emission\", \"pollution\", \"recycle\", \"waste\", \"plant\", \"energy\", \"renewable\", \"water\", \"electricity\",\n",
    "    \"diversity\", \"employee\", \"women\", \"female\", \"human\", \"inclusion\", \"health\", \"safety\", \"security\",\n",
    "    # \"goal\", \"sustainable\", \"zero\", \"right\"\n",
    "    ]\n",
    "data_preprocessor = data_preprocessing.DataPreprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gc = data_preprocessor.clean_text_blocks(df_gc, \"tweet\", level=\"minimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_so = data_preprocessor.clean_text_blocks(df_so, \"Text Blocks\", level=\"minimal\")\n",
    "df_so = data_preprocessor.filter_text_blocks(df_so, \"Text Blocks\", keep_only_size=(0, 300), keep_only_keywords=sustainability_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "df_gc[\"labels\"] = le.fit_transform(df_gc[\"label_binary\"])\n",
    "df_gc = df_gc.rename(columns={\"tweet\": \"text\"})\n",
    "df_gc = df_gc[[\"text\", \"labels\"]]\n",
    "\n",
    "df_gc_train, df_gc_test = sklearn.model_selection.train_test_split(\n",
    "    df_gc,\n",
    "    test_size=0.2,\n",
    "    stratify=df_gc[\"labels\"],\n",
    "    random_state=7\n",
    ")\n",
    "print(\"Train Set Size:\", df_gc_train.shape)\n",
    "print(df_gc_train[\"labels\"].value_counts())\n",
    "print(\"Test Set Size:\", df_gc_test.shape)\n",
    "print(df_gc_test[\"labels\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_so[\"labels\"] = df_so[\"Goal\"].notnull().apply(lambda x: 1 if x else 0)\n",
    "df_so = df_so.rename(columns={\"Text Blocks\": \"text\", \"Goal\": \"labels\"})\n",
    "df_so = df_so[[\"text\", \"labels\"]]\n",
    "df_so = df_so.drop_duplicates(subset=[\"text\"])\n",
    "\n",
    "df_so_train, df_so_test = sklearn.model_selection.train_test_split(\n",
    "    df_so,\n",
    "    test_size=0.2,\n",
    "    stratify=df_so[\"labels\"],\n",
    "    random_state=7\n",
    ")\n",
    "\n",
    "print(\"Train Set Size:\", df_so_train.shape)\n",
    "print(df_so_train[\"labels\"].value_counts())\n",
    "print(\"Test Set Size:\", df_so_test.shape)\n",
    "print(df_so_test[\"labels\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Baseline 1: BERTClaimBuster ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer_model.TransformerModel(name=\"Nithiwat/bert-base_claimbuster\", load_from=\"Nithiwat/bert-base_claimbuster\")\n",
    "pipe = model.load_pipeline(number_of_labels=3)\n",
    "predictions = pipe(df_gc_test[\"text\"].tolist())\n",
    "y_predicted = [(1 - p[0][\"score\"]) >= 0.5 for p in predictions]\n",
    "evaluation_metrics = sklearn.metrics.classification_report(df_gc_test[\"labels\"], y_predicted)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer_model.TransformerModel(name=\"Nithiwat/bert-base_claimbuster\", load_from=\"Nithiwat/bert-base_claimbuster\")\n",
    "pipe = model.load_pipeline(number_of_labels=3)\n",
    "predictions = pipe(df_so_test[\"text\"].tolist())\n",
    "y_predicted = [(1 - p[0][\"score\"]) >= 0.5 for p in predictions]\n",
    "evaluation_metrics = sklearn.metrics.classification_report(df_so_test[\"labels\"], y_predicted)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Baseline 2: TFIDF + Random Forest ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(min_df=3, max_df=0.5, analyzer=\"word\")\n",
    "x_train_vectorized = vectorizer.fit_transform(df_gc_train[\"text\"])\n",
    "x_test_vectorized = vectorizer.transform(df_gc_test[\"text\"])\n",
    "\n",
    "parameters_grid = {\n",
    "    \"n_estimators\": range(50, 550, 50),\n",
    "}\n",
    "model = sklearn.model_selection.GridSearchCV(sklearn.ensemble.RandomForestClassifier(), \n",
    "                                             parameters_grid, scoring=\"f1_macro\", cv=4, n_jobs=-1)\n",
    "model.fit(x_train_vectorized, df_gc_train[\"labels\"])\n",
    "print(\"Best found hyperparameters of Random Forest classfier = {}\".format(model.best_params_))\n",
    "\n",
    "y_predicted = model.predict(x_test_vectorized)\n",
    "evaluation_metrics = sklearn.metrics.classification_report(df_gc_test[\"labels\"], y_predicted)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(min_df=3, max_df=0.5, analyzer=\"word\")\n",
    "x_train_vectorized = vectorizer.fit_transform(df_so_train[\"text\"])\n",
    "x_test_vectorized = vectorizer.transform(df_so_test[\"text\"])\n",
    "\n",
    "parameters_grid = {\n",
    "    \"n_estimators\": range(50, 550, 50),\n",
    "}\n",
    "model = sklearn.model_selection.GridSearchCV(sklearn.ensemble.RandomForestClassifier(), \n",
    "                                             parameters_grid, scoring=\"f1_macro\", cv=4, n_jobs=-1)\n",
    "model.fit(x_train_vectorized, df_so_train[\"labels\"])\n",
    "print(\"Best found hyperparameters of Random Forest classfier = {}\".format(model.best_params_))\n",
    "\n",
    "y_predicted = model.predict(x_test_vectorized)\n",
    "evaluation_metrics = sklearn.metrics.classification_report(df_so_test[\"labels\"], y_predicted)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Baseline 3: Bin_RoBERTa ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer_model.TransformerModel(name=\"roberta-base\", epochs=10, learning_rate=2e-5, batch_size=32, weight_decay=0.0, save_to=\"../models\")\n",
    "model.fit(df_gc_train, df_gc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer_model.TransformerModel(name=\"roberta-base\", epochs=10, learning_rate=2e-5, batch_size=32, weight_decay=0.0, save_to=\"../models\")\n",
    "model.fit(df_so_train, df_so_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Training and Testing Our Model on the Dataset ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer_model.TransformerModel(epochs=10, learning_rate=1e-4, save_to=\"../models\")\n",
    "model.fit(df_gc_train, df_gc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer_model.TransformerModel(save_to=\"../models\")\n",
    "model.fit(df_so_train, df_so_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
